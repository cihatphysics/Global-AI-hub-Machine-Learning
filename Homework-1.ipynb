{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMEWORK-1    Cihat Güleryüz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Machine Learning\n",
    "ML : \n",
    "Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it to learn for themselves.\n",
    "Machine learning enables analysis of massive quantities of data. While it generally delivers faster, more accurate results in order to identify profitable opportunities or dangerous risks, it may also require additional time and resources to train it properly. Combining machine learning with AI and cognitive technologies can make it even more effective in processing large volumes of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Difference between Supervised and Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Supervised uses Input data which is known and labeled data as input. But Unsupervised uses Input data which is  unknown data as input.\n",
    "\n",
    "-Supervised has computational complexity. while Unsupervised has less Computational Complexity.\n",
    "\n",
    "-Real time of Supervised uses off-line analysis, whereas Unsupervised uses real time analysis of data\n",
    "\n",
    "-For supervised Number of classes are known, while in unsupervised\tnumber of classes are not known.\n",
    "\n",
    "-Accuracy of results for supervised are accurate and reliable results,  for unsupervised has moderate accurate and reliable results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example of supervised learning algorithms :\n",
    "    \n",
    ".Linear Regression\n",
    "\n",
    ".Logistic Regression\n",
    "\n",
    ".K-Nearest Neighbors\n",
    "\n",
    ".Decision Tree\n",
    "\n",
    ".Random Forest\n",
    "\n",
    ".Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Examples of unsupervised learning algorithms:\n",
    "    \n",
    ".Dimension Reduction\n",
    "\n",
    ".Density Estimation\n",
    "\n",
    ".Market Basket Analysis\n",
    "\n",
    ".Generative adversarial networks (GANs)\n",
    "\n",
    ".Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Validation set and Test set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Dataset: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Dataset: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use them in our system. Because,the validation set is used to estimate prediction error for model selection; the test set is used for assessment of the generalization error of the final chosen model. Ideally, the test set should be kept in a “vault,” and be brought out only at the end of the data analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. the main preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. Acquire the dataset\n",
    "To build and develop Machine Learning models, you must first acquire the relevant dataset. This dataset will be comprised of data gathered from multiple and disparate sources which are then combined in a proper format to form a dataset. Dataset formats differ according to use cases. For instance, a business dataset will be entirely different from a medical dataset. While a business dataset will contain relevant industry and business data, a medical dataset will include healthcare-related data.\n",
    "\n",
    "2. Import all the crucial libraries\n",
    "Since Python is the most extensively used and also the most preferred library by Data Scientists around the world, we’ll show you how to import Python libraries for data preprocessing in Machine Learning. Read more about Python libraries for Data Science here. The predefined Python libraries can perform specific data preprocessing jobs. The three core Python libraries used for this data preprocessing in Machine Learning are:\n",
    "\n",
    "NumPy – NumPy is the fundamental package for scientific calculation in Python. Hence, it is used for inserting any type of mathematical operation in the code. Using NumPy, you can also add large multidimensional arrays and matrices in your code. \n",
    "Pandas – Pandas is an excellent open-source Python library for data manipulation and analysis. It is extensively used for importing and managing the datasets. It packs in high-performance, easy-to-use data structures and data analysis tools for Python.\n",
    "Matplotlib – Matplotlib is a Python 2D plotting library that is used to plot any type of charts in Python. It can deliver publication-quality figures in numerous hard copy formats and interactive environments across platforms (IPython shells, Jupyter notebook, web application servers, etc.). \n",
    "\n",
    "3. Import the dataset\n",
    "In this step, you need to import the dataset/s that you have gathered for the ML project at hand. However, before you can import the dataset/s, you must set the current directory as the working directory. You can set the working directory in Spyder IDE in three simple steps:\n",
    "\n",
    "a.Save your Python file in the directory containing the dataset.\n",
    "b.Go to File Explorer option in Spyder IDE and choose the required directory.\n",
    "c.Now, click on the F5 button or Run option to execute the file.\n",
    "\n",
    "4. Identifying and handling the missing values\n",
    "In data preprocessing, it is pivotal to identify and correctly handle the missing values, failing to do this, you might draw inaccurate and faulty conclusions and inferences from the data. Needless to say, this will hamper your ML project. \n",
    "\n",
    "Basically, there are two ways to handle missing data:\n",
    "\n",
    "Deleting a particular row – In this method, you remove a specific row that has a null value for a feature or a particular column where more than 75% of the values are missing. However, this method is not 100% efficient, and it is recommended that you use it only when the dataset has adequate samples. You must ensure that after deleting the data, there remains no addition of bias. \n",
    "Calculating the mean – This method is useful for features having numeric data like age, salary, year, etc. Here, you can calculate the mean, median, or mode of a particular feature or column or row that contains a missing value and replace the result for the missing value. This method can add variance to the dataset, and any loss of data can be efficiently negated. Hence, it yields better results compared to the first method (omission of rows/columns). Another way of approximation is through the deviation of neighbouring values. However, this works best for linear data.\n",
    "\n",
    "5. Encoding the categorical data\n",
    "Categorical data refers to the information that has specific categories within the dataset. In the dataset cited above, there are two categorical variables – country and purchased.\n",
    "\n",
    "Machine Learning models are primarily based on mathematical equations. Thus, you can intuitively understand that keeping the categorical data in the equation will cause certain issues since you would only need numbers in the equations.\n",
    "\n",
    "How to encode the country variable?\n",
    "\n",
    "As seen in our dataset example, the country column will cause problems, so you must convert it into numerical values. To do so, you can use the LabelEncoder() class from the sci-kit learn library.\n",
    "\n",
    "6. Splitting the dataset\n",
    "Every dataset for Machine Learning model must be split into two separate sets – training set and test set. \n",
    "\n",
    "data preprocessing\n",
    "\n",
    "Source\n",
    "\n",
    "Training set denotes the subset of a dataset that is used for training the machine learning model. Here, you are already aware of the output. A test set, on the other hand, is the subset of the dataset that is used for testing the machine learning model. The ML model uses the test set to predict outcomes. \n",
    "\n",
    "Usually, the dataset is split into 70:30 ratio or 80:20 ratio. This means that you either take 70% or 80% of the data for training the model while leaving out the rest 30% or 20%. The splitting process varies according to the shape and size of the dataset in question. \n",
    "\n",
    " To split the dataset, you have to write the following line of code – \n",
    "\n",
    " from sklearn.model_selection import train_test_split  \n",
    "\n",
    "x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.2, random_state=0)  \n",
    "\n",
    "Here, the first line splits the arrays of the dataset into random train and test subsets. The second line of code includes four variables:\n",
    "\n",
    "x_train – features for the training data\n",
    "x_test – features for the test data\n",
    "y_train – dependent variables for training data\n",
    "y_test – independent variable for testing data\n",
    "Thus, the train_test_split() function includes four parameters, the first two of which are for arrays of data. The test_size function specifies the size of the test set. The test_size maybe .5, .3, or .2 – this specifies the dividing ratio between the training and test sets. The last parameter, “random_state” sets seed for a random generator so that the output is always the same. \n",
    "\n",
    "7. Feature scaling\n",
    "Feature scaling marks the end of the data preprocessing in Machine Learning. It is a method to standardize the independent variables of a dataset within a specific range. In other words, feature scaling limits the range of variables so that you can compare them on common grounds.\n",
    "\n",
    "In the dataset, you can notice that the age and salary columns do not have the same scale. In such a scenario, if you compute any two values from the age and salary columns, the salary values will dominate the age values and deliver incorrect results. Thus, you must remove this issue by performing feature scaling for Machine Learning.\n",
    "\n",
    "Most ML models are based on Euclidean Distance, which is represented as:\n",
    "\n",
    "       d(A,B)= square[(x_1-x_2)^2-(y_1-y_2)^2]\n",
    "        \n",
    "        You can perform feature scaling in Machine Learning in two ways:\n",
    "\n",
    "Standardization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Normalization\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For our dataset, we will use the standardization method. To do so, we will import StandardScaler class of the sci-kit-learn library using the following line of code:\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "The next step will be to create the object of StandardScaler class for independent variables. After this, you can fit and transform the training dataset using the following code:\n",
    "\n",
    "st_x= StandardScaler()  \n",
    "\n",
    "x_train= st_x.fit_transform(x_train) \n",
    "\n",
    "For the test dataset, you can directly apply transform() function (you need not use the fit_transform() function because it is already done in training set). The code will be as follows – \n",
    "\n",
    "x_test= st_x.transform(x_test) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Why we need to prepare our data?\n",
    "\n",
    "Because, Data preprocessing in Machine Learning is a crucial step that helps enhance the quality of data to promote the extraction of meaningful insights from the data. Data preprocessing in Machine Learning refers to the technique of preparing (cleaning and organizing) the raw data to make it suitable for a building and training Machine Learning models. In simple words, data preprocessing in Machine Learning is a data mining technique that transforms raw data into an understandable and readable format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Discrete and Continuous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at a set of numbers, they are typically discrete (countable) variables or continuous (measurable) variables. How you study this data should differ based on which group it falls into. This will certainly affect how it is measured as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we collect a set of round, defined numbers, they would fall in place on a graph something. Discrete data relates to individual, countable items.\n",
    "\n",
    "When we measure a certain stream of data with a complex range of results, these findings would be charted with a line as a range of data. Continuous data relates to change over time, involving concepts that are not simply countable but require detailed measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
